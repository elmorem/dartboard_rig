# Dartboard RAG - Quick Start Guide

## üöÄ Get Up and Running in 5 Minutes

### What You Have Right Now

```
‚úÖ Complete Dartboard Algorithm
‚úÖ 6 Passing Tests
‚úÖ Vector Store Integration (FAISS, Pinecone)
‚úÖ Synthetic Dataset Generation
‚úÖ Comprehensive Metrics
```

### What You Can Do Right Now

#### 1. Run Basic Retrieval (30 seconds)
```bash
source .venv/bin/activate
python demo_dartboard.py
```

**Output:**
```
üéØ Dartboard RAG Demo

Loading embedding model...
‚úì Model loaded (dim=384)

Top 3 results:
1. [Score: 0.2111] The Dartboard algorithm optimizes...
2. [Score: 0.9558] Information retrieval is crucial...
3. [Score: 0.9251] FastAPI is a modern web framework...
```

#### 2. Run Full Evaluation (2 minutes)
```bash
python demo_dartboard_evaluation.py
```

**See:**
- NDCG, MAP, Precision@K metrics
- Comparison across different œÉ values
- Diversity analysis

#### 3. Test Deduplication (1 minute)
```bash
python test_redundancy.py
```

**Validates:**
- Perfect 10/10 unique passage selection
- High diversity (>1.0 mean distance)

---

## üìä What the Tests Show

### Performance
- **Throughput:** 5,790 passages/sec (500 corpus size)
- **Latency:** 85ms retrieval time (p95)
- **Scalability:** Constant time with corpus growth

### Quality
- **Precision@1:** 100% on Q&A dataset
- **NDCG:** 0.41 (synthetic), 0.34 (Q&A)
- **Diversity:** 1.00 mean pairwise distance

### Deduplication
- **Success Rate:** 10/10 unique passages from 50 redundant
- **Works Across:** All œÉ values (0.5, 1.0, 2.0, 5.0)

---

## üèóÔ∏è To Build a Full RAG System

### Week 1: Core Pipeline
```python
# Document ‚Üí Chunks ‚Üí Embeddings ‚Üí Vector Store
loader = PDFLoader()
chunker = RecursiveChunker(chunk_size=512)
vector_store = FAISSStore(embedding_dim=384)

docs = loader.load("whitepaper.pdf")
chunks = chunker.chunk(docs[0]["text"])
vector_store.add(chunks)
```

**Need to implement:**
- `PDFLoader` (2 days)
- `RecursiveChunker` (1 day)
- Already have: `FAISSStore` ‚úÖ

### Week 2: LLM Integration
```python
# Query ‚Üí Retrieve ‚Üí Generate
retriever = HybridRetriever(vector_store, model, config)
generator = RAGGenerator(openai_client)

result = retriever.retrieve("How does Dartboard work?")
answer = generator.generate(query, result.chunks)
```

**Need to implement:**
- `RAGGenerator` (2 days)
- Already have: `HybridRetriever` ‚úÖ

### Week 3: API & Deployment
```python
# FastAPI endpoints
@app.post("/query")
async def query(request: QueryRequest):
    chunks = retriever.retrieve(request.query)
    answer = generator.generate(request.query, chunks)
    return {"answer": answer, "sources": chunks}
```

**Need to implement:**
- FastAPI routes (2 days)
- Docker deployment (1 day)

---

## üí∞ Cost to Run

### Development (Your Laptop)
```
Vector Store: FAISS (free, local)
Embeddings: SentenceTransformer (free, local)
LLM: OpenAI API (pay per use)

Cost: ~$0.002 per query
```

### Production (Cloud)
```
Server: AWS EC2 t3.large          $60/mo
Vector Store: Pinecone            $70/mo
LLM: OpenAI GPT-4                $300/mo (10K queries)
Database: RDS PostgreSQL          $40/mo
Total:                           ~$470/mo
```

**Optimizations:**
- Use GPT-3.5 instead of GPT-4: Save $200/mo
- Use FAISS instead of Pinecone: Save $70/mo
- **Total MVP cost:** ~$200/mo

---

## üéØ Recommended Path

### Option A: Quick Prototype (1 week)
```
1. Keep FAISS (local vector store)
2. Add OpenAI GPT-3.5 (cheap LLM)
3. Simple PDF loader
4. Basic chunking
5. FastAPI with /query endpoint

Result: Working RAG demo
Cost: <$100/mo
```

### Option B: Production Ready (3-4 weeks)
```
1. Pinecone vector store
2. OpenAI GPT-4 or Claude 3.5
3. Multiple document loaders
4. Smart chunking with overlap
5. Full FastAPI with auth
6. Monitoring + caching
7. Kubernetes deployment

Result: Scalable RAG system
Cost: ~$500/mo
```

---

## üìà Performance Expectations

### Current (Dartboard Only)
```
Retrieval: 85ms
Throughput: 5,790 p/s (at 500 corpus)
Memory: ~200MB (model + vectors)
```

### With LLM Added
```
Retrieval: 85ms
Generation: 2,000-4,000ms (GPT-4)
End-to-End: ~2.5s total
Throughput: 20-40 queries/sec (per instance)
```

### At Scale (with caching)
```
Cache Hit: 5ms (90% faster)
Cache Miss: 2,500ms
Average: ~250ms (assuming 30% hit rate)
```

---

## üî• Quick Wins

### Immediate Improvements
1. **Add Streaming** - Show partial answers as they generate
2. **Cache Common Queries** - Redis for 30%+ hit rate
3. **Batch Embeddings** - 10x faster ingestion
4. **Async Processing** - Non-blocking document upload

### Code Examples

**Streaming Response:**
```python
async def stream_answer(query: str):
    chunks = retriever.retrieve(query)
    
    async for token in llm.stream(query, chunks):
        yield f"data: {token}\n\n"
```

**Redis Caching:**
```python
cache_key = hashlib.md5(query.encode()).hexdigest()
cached = redis.get(cache_key)

if cached:
    return json.loads(cached)  # 5ms

result = retriever.retrieve(query)
redis.setex(cache_key, 3600, json.dumps(result))
return result  # 2500ms, cached for next time
```

---

## üß™ Test Before You Build

Use the existing tests to validate your integration:

```python
# After building your RAG system, test it:
from test_qa_dataset import main as test_qa
from test_scalability import main as test_scale

# Validate quality
test_qa()  # Should get >80% Precision@1

# Validate performance  
test_scale()  # Should handle 100+ QPS
```

---

## üìö Read Next

1. **[Test Report](DARTBOARD_TEST_REPORT.md)** - See what's validated
2. **[Integration Plan](RAG_INTEGRATION_PLAN.md)** - Full architecture
3. **[Implementation Summary](RAG_IMPLEMENTATION_SUMMARY.md)** - What to build

---

## ‚ùì FAQ

**Q: Can I use a different embedding model?**  
A: Yes! Just swap `SentenceTransformerModel("all-MiniLM-L6-v2")` with any compatible model. OpenAI's `text-embedding-3-large` is excellent.

**Q: Does this work with local LLMs?**  
A: Yes! Replace OpenAI client with Ollama/vLLM. Expect slower generation (5-10s vs 2-3s).

**Q: How many documents can it handle?**  
A: FAISS: ~1M vectors on 8GB RAM. Pinecone: unlimited (managed cloud).

**Q: What about multi-language support?**  
A: Use `paraphrase-multilingual-MiniLM-L12-v2` for 50+ languages.

**Q: Can I customize œÉ (sigma) per query?**  
A: Yes! Pass it in the API request. Higher œÉ = more diversity.

---

**Ready to build?** Start with Option A (Quick Prototype) ‚Üí Deploy ‚Üí Iterate

*Quick Start Guide - 2025-11-20*
